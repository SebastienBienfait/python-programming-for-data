{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM72cUk7oyWBMK8CDmlG1Rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futureCodersSE/python-programming-for-data/blob/main/Projects/Bus_Data_dataset_simplification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a simplified dataset\n",
        "----\n"
      ],
      "metadata": {
        "id": "IEP-Rt7W8Ckk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "---\n",
        "As the data refreshes every 10 seconds, in order to get a complete overview of bus emissions in the AQMA, it is necessary to pull the data every few minutes. The result however is many many JSON files. \n",
        "\n",
        "In order to analyse the data, we need to simplify the data by creating a large  dataset which only contains the information we really need. \n",
        "\n",
        "To do this:\n",
        "* find all the individual filenames\n",
        "* read all the files into dataframes\n",
        "* create a list of dataframes\n",
        "* concatenate the dataframes to create 1 big dataframe\n",
        "* remove unnecessary columns \n",
        "* remove duplicate rows\n",
        "* remove all rows which are not Euro III buses \n",
        "* remove all rows which are not in the AQMA Rainham boundary (listed in the presentation)\n"
      ],
      "metadata": {
        "id": "urv17hCb8KEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the filenames\n",
        "---\n",
        "\n",
        "**Important do not skip**  \n",
        "First you will need to add the data to your drive:   \n",
        "Click https://drive.google.com/drive/folders/1XmGyJqykj44wt7Eieb6vEoWeY6kH0cwN?usp=sharing\n",
        "\n",
        "1. add a shortcut to the folder to your drive (click the little arrow next to the folder name OneHourOfData\n",
        "2. When you go to My Drive, there should be a folder called **OneHourOfData**\n",
        "\n",
        "### Run the following code \n",
        "\n",
        "Due to all the data files having long names, we don't know the exact filenames.\n",
        "Run the code cell below to generate a list of the filenames in the data folder.\n",
        " \n",
        "It will ask for permission to access your Google Drive, click accept"
      ],
      "metadata": {
        "id": "ppB5NEwmhJnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json \n",
        "from google.colab import drive\n",
        "\n",
        "def mount_drive():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  project_dir = \"/content/drive/MyDrive/OneHourOfData\"\n",
        "  return project_dir\n",
        "\n",
        "def unmount_drive():\n",
        "  drive.flush_and_unmount()\n",
        "  print('Drive Unmounted')\n",
        "\n",
        "def get_file_names(project_dir):\n",
        "  path = os.path.join(os.getcwd(),project_dir)\n",
        "  filenames = [os.path.join(path,i) for i in os.listdir(path) if os.path.isfile(os.path.join(path,i))]\n",
        "  return filenames\n",
        "\n",
        "\n",
        "project_dir = mount_drive()\n",
        "\n",
        "filenames = get_file_names(project_dir)\n",
        "filenames = filenames[:6]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3t9KX-ChaAe",
        "outputId": "c863f3ec-743f-4999-df7a-d1d80b14f910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you leave the worksheet\n",
        "---\n",
        "**Run the following code**\n",
        "\n",
        "When you finish the exercises or are leaving the worksheet to come back to later make sure to run the following code to unmount your google drive \n"
      ],
      "metadata": {
        "id": "Q7WuJYu1djGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unmount_drive()"
      ],
      "metadata": {
        "id": "C746Ovt5diZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 \n",
        "---\n",
        "\n",
        "Create a dataframe from the first json file in the `filenames` list. \n",
        "\n",
        "*(hint: use pd.read_json())*"
      ],
      "metadata": {
        "id": "PIynWFZQhGA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G--ZbH1B77dl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "---\n",
        "Create a list of dataframes called `df_list` from all the filenames in the `filenames` list\n",
        "\n",
        "*hint: you will need to use a for loop*"
      ],
      "metadata": {
        "id": "0pieG4mRydAz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldB8lDjvyukZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 \n",
        "---\n",
        "\n",
        "Create one big dataframe, by appending all the dataframes from task 2 together\n",
        "\n",
        "*hint: you will need to create an empty dataframe and use a for loop*"
      ],
      "metadata": {
        "id": "QF7OPv-NyvAj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GLmuCHbzJr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4\n",
        "---\n",
        "Create a new dataframe which is normalized (using pd.json_normalize()) by the column `MonitoredVehicleJourney` "
      ],
      "metadata": {
        "id": "b7tJKDpgzkTi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_sVvlEmnz35q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5\n",
        "---\n",
        "Copy the column `RecordedAtTime` from the first big dataframe to the new dataframe you created in Task 4\n",
        "\n",
        "*hint: you will need to convert the old column to a list*\n",
        "\n",
        "**Expected output:** 18 columns "
      ],
      "metadata": {
        "id": "UAWCw1Rxz4Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNn7vfmt0iSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6\n",
        "---\n",
        "Remove the columns `\"DirectionRef\", \"PublishedLineName\", \"OperatorRef\", \"OriginRef\", \"DestinationRef\", \"DestinationAimedArrivalTime\", \"Bearing\", \"BlockRef\", \"FramedVehicleJourneyRef.DataFrameRef\", 'FramedVehicleJourneyRef.DatedVehicleJourneyRef'`"
      ],
      "metadata": {
        "id": "z4JDEFGjzKq9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CYX25yd0iz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7\n",
        "---\n"
      ],
      "metadata": {
        "id": "93xTxhF304Ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicate rows - only remove them if they are the same in all columns "
      ],
      "metadata": {
        "id": "lc3BWJCEhVEN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZj7THXK064Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8\n",
        "---\n",
        "Remove all rows which are not of **Euro III** standard.   \n",
        "To do this:\n"
      ],
      "metadata": {
        "id": "0ImtvQHyhhR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8.1\n",
        "--- \n",
        "* read into a new dataframe called `regs_emissions` the vehicle registrations dataset from this link: \"https://raw.githubusercontent.com/futureCodersSE/python-programming-for-data/main/Datasets/bus_regs.csv\"\n"
      ],
      "metadata": {
        "id": "-N8elfh_n56A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H37qs7hgnzJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8.2 \n",
        "---\n",
        "Create a list from the `regs_emissions` dataframe containing all the registrations from `Last tracked` which are Euro III\n",
        "\n",
        "\n",
        "*hint: to_list()*  \n",
        "**Expected Output:**\n",
        "\n",
        "euro3 list has length 56  \n",
        "euro3 list first entry is 1607"
      ],
      "metadata": {
        "id": "feLNzr5So-GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fUs19_1YtVBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8.3\n",
        "---\n",
        "\n",
        "Create a new dataframe called `euro3_buses` which contains only rows where the buses are Euro III standard\n",
        "\n",
        "* find rows where `VehicleRefs` is in the euro III registrations list from 8.2 "
      ],
      "metadata": {
        "id": "kXnV4pE6u_IY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwXxkn6oxQKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 9\n",
        "---\n",
        "We need to remove all rows that are not within the boundary of Rainham High Street (the AQMA) \n",
        "\n",
        "The bounding box for the latitude and longitude is as follows:\n",
        "```\n",
        "Max Lat 51.364935                                 Max Lat 51.364935\n",
        "Min Long: 0.603210 ------------------------------ Max Long 0.617510  \n",
        "                   |                            |  \n",
        "                   |                            |  \n",
        "                   |                            |  \n",
        "                   |                            |  \n",
        "                   |                            |\n",
        "Min Lat 51.361462  ------------------------------ Min Lat 51.361462\n",
        "Min Long 0.603210                                 Max Long 0.617510\n",
        "```\n",
        "Therefore, to be in the boundary:\n",
        "* the longitude must be between 0.603210 and 0.617510\n",
        "* the latitude must be between 51.361462 and 51.364935\n",
        "\n",
        "Remove all rows from the `euro3_buses` df where the latitude and longitude are not within the max and min limits\n",
        "* You will first need to convert the latitude and longitude columns to floats\n",
        "\n",
        "*hint: use pd.to_numeric()* "
      ],
      "metadata": {
        "id": "os0AMrSGxyWG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-BtLRop-x6oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 10 CHALLENGE  \n",
        "--- \n",
        "Can you do task 8 but with this different method:\n",
        "\n",
        "\n",
        "Create a new column which has the corresponding emissions standard for each `VehicleRef` \n",
        "\n",
        "* repeat task 8.2 but for all emissions standards (you should end up with 4 lists, one for each emissions class)\n",
        "* write a function which takes a dataframe as a parameter \n",
        "* use a series of if/elif statements which checks if the column `VehicleRef` is in each list\n",
        "* return the corresponsing emission as a string for if/elif statement\n",
        "* else return None \n",
        "* apply the function to create a new column called `Emissions_standard` to the dataframe from Task 7 \n",
        "* create a new dataframe which keeps only the rows where the `Emissons_standard` is \"Euro III\"  \n",
        "\n",
        "*hint: df[new_col_name] = df.apply(function_name, axis=1)*\n",
        "\n",
        "How to use apply:\n",
        "\n",
        "The apply() function will apply the function to each line of code - essentially it iterates through the column like a for loop and runs the if statements on each row. \n",
        "\n",
        "Heres an example:\n",
        "\n",
        "```\n",
        "def encode_bmi(df)\n",
        "  if df['bmi'] >= 25:\n",
        "\n",
        "    return 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0\n",
        "\n",
        "df[\"bmi\"] = df.apply(encode_bmi, axis=1)\n",
        "```\n",
        "\n",
        "This code will look at each row of the bmi column in the df and replace the value with 1 if the old value is above 25 and 0 if it's below. \n",
        "\n",
        "If you set the function to a new column name, it will do the same but rather than replace the old value in the bmi column, it will create a new column and put either a 1 or 0 in each row of the new column depending on the if criteria, but the original column will remain the same.\n",
        "\n",
        "eg. \n",
        "```\n",
        "def function_name(df):\n",
        "  if df[old_col] = condition:\n",
        "    return \"condition_met\"\n",
        "  else:\n",
        "    return \"condition_not\"\n",
        "\n",
        "df[new_col_name] = df.apply(function_name, axis=1)\n",
        "```\n"
      ],
      "metadata": {
        "id": "JBugkZK_tV9D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuvzBpj5pTHK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}